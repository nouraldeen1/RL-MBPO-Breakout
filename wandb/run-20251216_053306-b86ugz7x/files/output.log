wandb: Detected [agents] in use.
wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/

Starting training for 10,000,000 timesteps...
============================================================
Step:      1,000 | Episodes:     21 | Mean Reward (100):    0.52 | Best:    -inf
Step:      2,000 | Episodes:     50 | Mean Reward (100):    0.38 | Best:    -inf
Step:      3,000 | Episodes:     78 | Mean Reward (100):    0.33 | Best:    -inf
C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\gymnasium\wrappers\rendering.py:283: UserWarning: [33mWARN: Overwriting existing videos at D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  logger.warn(
[Video] Saved episode video at step 3840 (reward: 2.0)
Step:      4,000 | Episodes:    104 | Mean Reward (100):    0.35 | Best:    0.35
Step:      5,000 | Episodes:    131 | Mean Reward (100):    0.31 | Best:    0.35
Step:      6,000 | Episodes:    163 | Mean Reward (100):    0.29 | Best:    0.35
Step:      7,000 | Episodes:    189 | Mean Reward (100):    0.27 | Best:    0.35
[Video] Saved episode video at step 7344 (reward: 0.0)
Step:      8,000 | Episodes:    227 | Mean Reward (100):    0.20 | Best:    0.35
Step:      9,000 | Episodes:    253 | Mean Reward (100):    0.22 | Best:    0.35
Step:     10,000 | Episodes:    285 | Mean Reward (100):    0.15 | Best:    0.35
Traceback (most recent call last):
  File ".\src\main.py", line 437, in <module>
    main()
  File ".\src\main.py", line 433, in main
    train(config, args)
  File ".\src\main.py", line 235, in train
    action, action_info = agent.get_action(obs[i], deterministic=False)
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\agents.py", line 148, in get_action
    action, log_prob, entropy, value = self.actor_critic.get_action_and_value(
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\models.py", line 795, in get_action_and_value
    entropy = dist.entropy()
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\distributions\categorical.py", line 148, in entropy
    return -p_log_p.sum(-1)
KeyboardInterrupt
Traceback (most recent call last):
  File ".\src\main.py", line 437, in <module>
    main()
  File ".\src\main.py", line 433, in main
    train(config, args)
  File ".\src\main.py", line 235, in train
    action, action_info = agent.get_action(obs[i], deterministic=False)
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\agents.py", line 148, in get_action
    action, log_prob, entropy, value = self.actor_critic.get_action_and_value(
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\models.py", line 795, in get_action_and_value
    entropy = dist.entropy()
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\distributions\categorical.py", line 148, in entropy
    return -p_log_p.sum(-1)
KeyboardInterrupt
