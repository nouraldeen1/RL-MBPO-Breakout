wandb: Detected [agents] in use.
wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/

Starting training for 100,000 timesteps...
============================================================
[ActionDebug] Step 0: {'action/0': 2, 'action/1': 0, 'action/2': 1, 'action/3': 1}
Step:      1,000 | Episodes:     21 | Mean Reward (100):    0.38 | Best:    -inf
[ActionDebug] Step 1000: {'action/0': 257, 'action/1': 231, 'action/2': 247, 'action/3': 269}
Step:      2,000 | Episodes:     49 | Mean Reward (100):    0.45 | Best:    -inf
[ActionDebug] Step 2000: {'action/0': 500, 'action/1': 451, 'action/2': 494, 'action/3': 559}
Step:      3,000 | Episodes:     80 | Mean Reward (100):    0.35 | Best:    -inf
[ActionDebug] Step 3000: {'action/0': 761, 'action/1': 681, 'action/2': 732, 'action/3': 830}
C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\gymnasium\wrappers\rendering.py:283: UserWarning: [33mWARN: Overwriting existing videos at D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\videos\eval-000101 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  logger.warn(
[Video] Saved episode video: videos\eval-episode-000101.mp4 (reward: 0.0)
Step:      4,000 | Episodes:    108 | Mean Reward (100):    0.36 | Best:    0.39
[ActionDebug] Step 4000: {'action/0': 1031, 'action/1': 946, 'action/2': 960, 'action/3': 1067}
[Training] Step 5000: policy_loss=0.0001, grad_norm=0.5192, entropy=1.3863
Step:      5,000 | Episodes:    143 | Mean Reward (100):    0.26 | Best:    0.39
[ActionDebug] Step 5000: {'action/0': 1271, 'action/1': 1211, 'action/2': 1217, 'action/3': 1305}
[Training] Step 5100: policy_loss=-0.1675, grad_norm=2.2264, entropy=1.1444
[Training] Step 5200: policy_loss=-0.0840, grad_norm=20.5606, entropy=0.7782
[Training] Step 5300: policy_loss=0.4666, grad_norm=13.5092, entropy=1.0718
[Training] Step 5400: policy_loss=-1.1070, grad_norm=14.6689, entropy=1.0582
[Training] Step 5500: policy_loss=-0.5884, grad_norm=9.7259, entropy=1.0486
[Training] Step 5600: policy_loss=-1.2810, grad_norm=13.2018, entropy=1.0973
[Training] Step 5700: policy_loss=0.3819, grad_norm=19.9005, entropy=1.0039
[Training] Step 5800: policy_loss=-1.3090, grad_norm=9.1306, entropy=1.0859
[Training] Step 5900: policy_loss=0.1900, grad_norm=8.4655, entropy=1.0767
[Training] Step 6000: policy_loss=-0.2412, grad_norm=11.0242, entropy=1.0493
Step:      6,000 | Episodes:    164 | Mean Reward (100):    0.32 | Best:    0.39
[ActionDebug] Step 6000: {'action/0': 1642, 'action/1': 1227, 'action/2': 1542, 'action/3': 1593}
[Training] Step 6100: policy_loss=-0.3568, grad_norm=9.4148, entropy=1.0730
[Training] Step 6200: policy_loss=-0.9654, grad_norm=5.1191, entropy=1.0930
[Training] Step 6300: policy_loss=-0.2070, grad_norm=21.2713, entropy=0.9454
[Training] Step 6400: policy_loss=1.1911, grad_norm=15.7380, entropy=1.0002
[Training] Step 6500: policy_loss=-1.1365, grad_norm=12.9276, entropy=1.0892
[Training] Step 6600: policy_loss=0.3967, grad_norm=12.7959, entropy=1.0356
[Training] Step 6700: policy_loss=-0.0566, grad_norm=0.5670, entropy=1.3698
[Training] Step 6800: policy_loss=-0.0708, grad_norm=0.9092, entropy=1.3631
Traceback (most recent call last):
  File ".\src\main.py", line 511, in <module>
    main()
  File ".\src\main.py", line 507, in main
    train(config, args)
  File ".\src\main.py", line 273, in train
    actions, _ = agent.get_action(obs, deterministic=False)
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\agents.py", line 155, in get_action
    actions, log_probs, entropies, values = self.actor_critic.get_action_and_value(
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\models.py", line 780, in get_action_and_value
    logits, value = self.forward(obs)
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\models.py", line 755, in forward
    features = self.encoder(obs)
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\models.py", line 114, in forward
    x = F.relu(self.fc(x))
KeyboardInterrupt
Traceback (most recent call last):
  File ".\src\main.py", line 511, in <module>
    main()
  File ".\src\main.py", line 507, in main
    train(config, args)
  File ".\src\main.py", line 273, in train
    actions, _ = agent.get_action(obs, deterministic=False)
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\agents.py", line 155, in get_action
    actions, log_probs, entropies, values = self.actor_critic.get_action_and_value(
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\models.py", line 780, in get_action_and_value
    logits, value = self.forward(obs)
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\models.py", line 755, in forward
    features = self.encoder(obs)
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\models.py", line 114, in forward
    x = F.relu(self.fc(x))
KeyboardInterrupt
