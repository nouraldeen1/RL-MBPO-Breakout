wandb: Detected [agents] in use.
wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/

Starting training for 100,000 timesteps...
============================================================
[ActionDebug] Step 0: {'action/0': 2, 'action/1': 0, 'action/2': 1, 'action/3': 1}
Step:      1,000 | Episodes:     31 | Mean Reward (100):    0.19 | Best:    -inf
[ActionDebug] Step 1000: {'action/0': 256, 'action/1': 231, 'action/2': 248, 'action/3': 269}
Step:      2,000 | Episodes:     60 | Mean Reward (100):    0.23 | Best:    -inf
[ActionDebug] Step 2000: {'action/0': 499, 'action/1': 451, 'action/2': 495, 'action/3': 559}
Step:      3,000 | Episodes:     88 | Mean Reward (100):    0.24 | Best:    -inf
[ActionDebug] Step 3000: {'action/0': 759, 'action/1': 681, 'action/2': 734, 'action/3': 830}
C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\gymnasium\wrappers\rendering.py:283: UserWarning: [33mWARN: Overwriting existing videos at D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\videos\eval-000100 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  logger.warn(
[Video] Saved episode video: videos\eval-episode-000100.mp4 (reward: 0.0)
Step:      4,000 | Episodes:    120 | Mean Reward (100):    0.24 | Best:    0.28
[ActionDebug] Step 4000: {'action/0': 1029, 'action/1': 946, 'action/2': 962, 'action/3': 1067}
Step:      5,000 | Episodes:    144 | Mean Reward (100):    0.31 | Best:    0.31
[ActionDebug] Step 5000: {'action/0': 1269, 'action/1': 1211, 'action/2': 1219, 'action/3': 1305}
Step:      6,000 | Episodes:    177 | Mean Reward (100):    0.26 | Best:    0.31
[ActionDebug] Step 6000: {'action/0': 1520, 'action/1': 1448, 'action/2': 1478, 'action/3': 1558}
C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\gymnasium\wrappers\rendering.py:283: UserWarning: [33mWARN: Overwriting existing videos at D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\videos\eval-000201 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  logger.warn(
[Video] Saved episode video: videos\eval-episode-000201.mp4 (reward: 0.0)
Step:      7,000 | Episodes:    209 | Mean Reward (100):    0.20 | Best:    0.31
[ActionDebug] Step 7000: {'action/0': 1760, 'action/1': 1712, 'action/2': 1723, 'action/3': 1809}
Step:      8,000 | Episodes:    243 | Mean Reward (100):    0.16 | Best:    0.31
[ActionDebug] Step 8000: {'action/0': 2025, 'action/1': 1945, 'action/2': 1966, 'action/3': 2068}
Step:      9,000 | Episodes:    276 | Mean Reward (100):    0.17 | Best:    0.31
[ActionDebug] Step 9000: {'action/0': 2278, 'action/1': 2191, 'action/2': 2216, 'action/3': 2319}
C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\gymnasium\wrappers\rendering.py:283: UserWarning: [33mWARN: Overwriting existing videos at D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\videos\eval-000300 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  logger.warn(
[Video] Saved episode video: videos\eval-episode-000300.mp4 (reward: 0.0)
Step:     10,000 | Episodes:    306 | Mean Reward (100):    0.15 | Best:    0.31
Step:     11,000 | Episodes:    340 | Mean Reward (100):    0.20 | Best:    0.31
Step:     12,000 | Episodes:    374 | Mean Reward (100):    0.18 | Best:    0.31
C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\gymnasium\wrappers\rendering.py:283: UserWarning: [33mWARN: Overwriting existing videos at D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\videos\eval-000400 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  logger.warn(
[Video] Saved episode video: videos\eval-episode-000400.mp4 (reward: 0.0)
Step:     13,000 | Episodes:    402 | Mean Reward (100):    0.23 | Best:    0.31
Step:     14,000 | Episodes:    434 | Mean Reward (100):    0.20 | Best:    0.31
Step:     15,000 | Episodes:    460 | Mean Reward (100):    0.25 | Best:    0.31
Step:     16,000 | Episodes:    494 | Mean Reward (100):    0.20 | Best:    0.31
C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\gymnasium\wrappers\rendering.py:283: UserWarning: [33mWARN: Overwriting existing videos at D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\videos\eval-000501 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  logger.warn(
[Video] Saved episode video: videos\eval-episode-000501.mp4 (reward: 0.0)
Step:     17,000 | Episodes:    519 | Mean Reward (100):    0.26 | Best:    0.31
Step:     18,000 | Episodes:    550 | Mean Reward (100):    0.23 | Best:    0.31
Step:     19,000 | Episodes:    577 | Mean Reward (100):    0.29 | Best:    0.31
C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\gymnasium\wrappers\rendering.py:283: UserWarning: [33mWARN: Overwriting existing videos at D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\videos\eval-000600 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  logger.warn(
[Video] Saved episode video: videos\eval-episode-000600.mp4 (reward: 0.0)
[Training] Step 20000: policy_loss=-0.0001, grad_norm=0.1887, entropy=1.3863
Step:     20,000 | Episodes:    603 | Mean Reward (100):    0.28 | Best:    0.31
[Training] Step 20100: policy_loss=-0.0009, grad_norm=0.4015, entropy=1.3862
[Training] Step 20200: policy_loss=-0.0008, grad_norm=0.3590, entropy=1.3860
[Training] Step 20300: policy_loss=0.0035, grad_norm=0.2122, entropy=1.3854
[Training] Step 20400: policy_loss=0.0006, grad_norm=0.1555, entropy=1.3858
[Training] Step 20500: policy_loss=-0.0009, grad_norm=0.2453, entropy=1.3860
[Training] Step 20600: policy_loss=0.0055, grad_norm=0.2904, entropy=1.3861
[Training] Step 20700: policy_loss=-0.0023, grad_norm=0.3203, entropy=1.3862
[Training] Step 20800: policy_loss=0.0017, grad_norm=0.2500, entropy=1.3862
[Training] Step 20900: policy_loss=-0.0004, grad_norm=0.1110, entropy=1.3863
[Training] Step 21000: policy_loss=-0.0002, grad_norm=0.0691, entropy=1.3863
Step:     21,000 | Episodes:    633 | Mean Reward (100):    0.28 | Best:    0.31
[Training] Step 21100: policy_loss=0.0002, grad_norm=0.0566, entropy=1.3863
[Training] Step 21200: policy_loss=-0.0001, grad_norm=0.1620, entropy=1.3863
[Training] Step 21300: policy_loss=0.0016, grad_norm=0.1844, entropy=1.3862
[Training] Step 21400: policy_loss=-0.0014, grad_norm=0.2219, entropy=1.3862
[Training] Step 21500: policy_loss=0.0023, grad_norm=0.3843, entropy=1.3860
[Training] Step 21600: policy_loss=-0.0028, grad_norm=0.3277, entropy=1.3855
[Training] Step 21700: policy_loss=0.0084, grad_norm=0.2586, entropy=1.3840
[Training] Step 21800: policy_loss=-0.0022, grad_norm=0.9253, entropy=1.3585
[Training] Step 21900: policy_loss=-0.0148, grad_norm=3.5879, entropy=1.0753
[Training] Step 22000: policy_loss=-0.5450, grad_norm=7.2555, entropy=0.8022
Step:     22,000 | Episodes:    671 | Mean Reward (100):    0.19 | Best:    0.31
[Training] Step 22100: policy_loss=-1.9635, grad_norm=13.4285, entropy=0.2403
[Training] Step 22200: policy_loss=0.5237, grad_norm=11.8729, entropy=0.4802
[Training] Step 22300: policy_loss=1.1409, grad_norm=4.7700, entropy=0.7451
[Training] Step 22400: policy_loss=-0.4356, grad_norm=11.8888, entropy=0.6342
[Training] Step 22500: policy_loss=-0.6028, grad_norm=9.3905, entropy=0.6971
[Training] Step 22600: policy_loss=-0.2830, grad_norm=27.8602, entropy=0.2815
[Training] Step 22700: policy_loss=-0.2629, grad_norm=31.1248, entropy=0.1120
C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\gymnasium\wrappers\rendering.py:283: UserWarning: [33mWARN: Overwriting existing videos at D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\videos\eval-000700 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  logger.warn(
[Video] Saved episode video: videos\eval-episode-000700.mp4 (reward: 1.0)
[Training] Step 22800: policy_loss=-0.1162, grad_norm=40.3283, entropy=0.0141
[Training] Step 22900: policy_loss=-0.9963, grad_norm=25.8130, entropy=0.0033
[Training] Step 23000: policy_loss=-2.1341, grad_norm=16.7347, entropy=0.0000
Step:     23,000 | Episodes:    711 | Mean Reward (100):    0.11 | Best:    0.31
[Training] Step 23100: policy_loss=-0.8864, grad_norm=14.8439, entropy=0.0000
[Training] Step 23200: policy_loss=-3.0649, grad_norm=2.0032, entropy=0.0000
[Training] Step 23300: policy_loss=-0.1090, grad_norm=12.5889, entropy=0.0000
[Training] Step 23400: policy_loss=0.9154, grad_norm=0.9664, entropy=0.0000
[Training] Step 23500: policy_loss=-0.3053, grad_norm=3.2250, entropy=0.0000
[Training] Step 23600: policy_loss=-0.5298, grad_norm=2.0636, entropy=0.0000
[Training] Step 23700: policy_loss=-0.9027, grad_norm=20.0863, entropy=0.0000
[Training] Step 23800: policy_loss=-1.2184, grad_norm=1.7162, entropy=0.0000
[Training] Step 23900: policy_loss=-0.4810, grad_norm=1.2892, entropy=0.0000
[Training] Step 24000: policy_loss=0.1647, grad_norm=2.0323, entropy=0.0000
Step:     24,000 | Episodes:    747 | Mean Reward (100):    0.12 | Best:    0.31
[Training] Step 24100: policy_loss=1.1242, grad_norm=4.0286, entropy=0.0000
[Training] Step 24200: policy_loss=-1.8171, grad_norm=0.7277, entropy=0.0000
[Training] Step 24300: policy_loss=-1.6504, grad_norm=7.5499, entropy=0.0000
[Training] Step 24400: policy_loss=0.6328, grad_norm=3.4618, entropy=0.0000
[Training] Step 24500: policy_loss=-2.1589, grad_norm=0.6926, entropy=0.0000
[Training] Step 24600: policy_loss=0.7459, grad_norm=1.1973, entropy=0.0000
[Training] Step 24700: policy_loss=0.0474, grad_norm=0.4543, entropy=0.0000
[Training] Step 24800: policy_loss=-1.7530, grad_norm=0.7727, entropy=0.0000
[Training] Step 24900: policy_loss=0.6231, grad_norm=7.6809, entropy=0.0000
[Training] Step 25000: policy_loss=-0.5811, grad_norm=19.9981, entropy=0.0000
Step:     25,000 | Episodes:    782 | Mean Reward (100):    0.18 | Best:    0.31
C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\gymnasium\wrappers\rendering.py:283: UserWarning: [33mWARN: Overwriting existing videos at D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\videos\eval-000800 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  logger.warn(
[Video] Saved episode video: videos\eval-episode-000800.mp4 (reward: 0.0)
Step:     26,000 | Episodes:    813 | Mean Reward (100):    0.40 | Best:    0.40
Traceback (most recent call last):
  File ".\src\main.py", line 511, in <module>
    main()
  File ".\src\main.py", line 507, in main
    train(config, args)
  File ".\src\main.py", line 359, in train
    update_metrics = agent.update()
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\agents.py", line 376, in update
    n_generated = self.generate_model_rollouts()
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\agents.py", line 269, in generate_model_rollouts
    self.model_buffer.buffer.add_batch(s_np, a_np, r_np, ns_np, d_np)
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\utils.py", line 157, in add_batch
    self._store_at_indices(indices, states, actions, rewards, next_states, dones)
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\utils.py", line 193, in _store_at_indices
    np.multiply(states, 255.0, out=states if states.flags.writeable else None)
KeyboardInterrupt
Traceback (most recent call last):
  File ".\src\main.py", line 511, in <module>
    main()
  File ".\src\main.py", line 507, in main
    train(config, args)
  File ".\src\main.py", line 359, in train
    update_metrics = agent.update()
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\agents.py", line 376, in update
    n_generated = self.generate_model_rollouts()
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\agents.py", line 269, in generate_model_rollouts
    self.model_buffer.buffer.add_batch(s_np, a_np, r_np, ns_np, d_np)
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\utils.py", line 157, in add_batch
    self._store_at_indices(indices, states, actions, rewards, next_states, dones)
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\utils.py", line 193, in _store_at_indices
    np.multiply(states, 255.0, out=states if states.flags.writeable else None)
KeyboardInterrupt
