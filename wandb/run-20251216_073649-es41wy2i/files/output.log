wandb: Detected [agents] in use.
wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/

Starting training for 100,000 timesteps...
============================================================
[ActionDebug] Step 0: {'action/0': 2, 'action/1': 0, 'action/2': 1, 'action/3': 1}
Step:      1,000 | Episodes:     30 | Mean Reward (100):    0.20 | Best:    -inf
[ActionDebug] Step 1000: {'action/0': 256, 'action/1': 231, 'action/2': 248, 'action/3': 269}
Step:      2,000 | Episodes:     55 | Mean Reward (100):    0.35 | Best:    -inf
[ActionDebug] Step 2000: {'action/0': 499, 'action/1': 451, 'action/2': 495, 'action/3': 559}
Step:      3,000 | Episodes:     85 | Mean Reward (100):    0.29 | Best:    -inf
[ActionDebug] Step 3000: {'action/0': 760, 'action/1': 681, 'action/2': 733, 'action/3': 830}
C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\gymnasium\wrappers\rendering.py:283: UserWarning: [33mWARN: Overwriting existing videos at D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\videos\eval-000100 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  logger.warn(
[Video] Saved episode video: videos\eval-episode-000100.mp4 (reward: 0.0)
Step:      4,000 | Episodes:    113 | Mean Reward (100):    0.32 | Best:    0.33
[ActionDebug] Step 4000: {'action/0': 1030, 'action/1': 946, 'action/2': 961, 'action/3': 1067}
[Training] Step 5000: policy_loss=-0.0002, grad_norm=0.0000, entropy=1.3863
Step:      5,000 | Episodes:    138 | Mean Reward (100):    0.34 | Best:    0.34
[ActionDebug] Step 5000: {'action/0': 1270, 'action/1': 1211, 'action/2': 1218, 'action/3': 1305}
[Training] Step 5100: policy_loss=0.0016, grad_norm=0.0000, entropy=1.3863
[Training] Step 5200: policy_loss=0.0001, grad_norm=0.0000, entropy=1.3862
[Training] Step 5300: policy_loss=-0.0151, grad_norm=0.0000, entropy=1.3851
[Training] Step 5400: policy_loss=-0.0162, grad_norm=0.0000, entropy=1.3808
[Training] Step 5500: policy_loss=-0.0189, grad_norm=0.0000, entropy=1.3050
[Training] Step 5600: policy_loss=0.2838, grad_norm=0.0000, entropy=0.8394
[Training] Step 5700: policy_loss=0.0715, grad_norm=0.0000, entropy=0.6622
[Training] Step 5800: policy_loss=-0.9547, grad_norm=0.0000, entropy=0.3189
[Training] Step 5900: policy_loss=0.3208, grad_norm=0.0000, entropy=0.1058
[Training] Step 6000: policy_loss=-0.3624, grad_norm=0.0000, entropy=0.0837
Step:      6,000 | Episodes:    171 | Mean Reward (100):    0.27 | Best:    0.37
[ActionDebug] Step 6000: {'action/0': 1500, 'action/1': 1337, 'action/2': 1337, 'action/3': 1830}
[Training] Step 6100: policy_loss=-0.1783, grad_norm=0.0000, entropy=0.0262
[Training] Step 6200: policy_loss=-1.9702, grad_norm=0.0000, entropy=0.0842
[Training] Step 6300: policy_loss=-0.2002, grad_norm=0.0000, entropy=0.1170
[Training] Step 6400: policy_loss=-0.9677, grad_norm=0.0000, entropy=0.1936
[Training] Step 6500: policy_loss=-2.3767, grad_norm=0.0000, entropy=0.0834
[Training] Step 6600: policy_loss=1.1888, grad_norm=0.0000, entropy=0.0913
[Training] Step 6700: policy_loss=1.5535, grad_norm=0.0000, entropy=0.2351
[Training] Step 6800: policy_loss=0.2592, grad_norm=0.0000, entropy=0.5213
[Training] Step 6900: policy_loss=-0.7290, grad_norm=0.0000, entropy=0.5345
C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\gymnasium\wrappers\rendering.py:283: UserWarning: [33mWARN: Overwriting existing videos at D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\videos\eval-000200 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  logger.warn(
[Video] Saved episode video: videos\eval-episode-000200.mp4 (reward: 0.0)
[Training] Step 7000: policy_loss=-0.6087, grad_norm=0.0000, entropy=0.6530
Step:      7,000 | Episodes:    202 | Mean Reward (100):    0.47 | Best:    0.47
[ActionDebug] Step 7000: {'action/0': 1595, 'action/1': 1337, 'action/2': 1337, 'action/3': 2735}
[Training] Step 7100: policy_loss=0.2227, grad_norm=0.0000, entropy=0.3814
[Training] Step 7200: policy_loss=-1.1750, grad_norm=0.0000, entropy=0.3298
[Training] Step 7300: policy_loss=-1.5229, grad_norm=0.0000, entropy=0.3140
[Training] Step 7400: policy_loss=-0.5666, grad_norm=0.0000, entropy=0.1936
[Training] Step 7500: policy_loss=-1.3269, grad_norm=0.0000, entropy=0.0006
[Training] Step 7600: policy_loss=-1.4723, grad_norm=0.0000, entropy=0.0000
[Training] Step 7700: policy_loss=-1.8733, grad_norm=0.0000, entropy=0.0000
[Training] Step 7800: policy_loss=-2.2702, grad_norm=0.0000, entropy=0.0000
[Training] Step 7900: policy_loss=0.8786, grad_norm=0.0000, entropy=0.0000
[Training] Step 8000: policy_loss=-0.4693, grad_norm=0.0000, entropy=0.0000
Step:      8,000 | Episodes:    239 | Mean Reward (100):    0.36 | Best:    0.47
[ActionDebug] Step 8000: {'action/0': 1644, 'action/1': 1337, 'action/2': 1337, 'action/3': 3686}
[Training] Step 8100: policy_loss=-1.5129, grad_norm=0.0000, entropy=0.0000
[Training] Step 8200: policy_loss=-0.3858, grad_norm=0.0000, entropy=0.0000
[Training] Step 8300: policy_loss=-2.5050, grad_norm=0.0000, entropy=0.0000
[Training] Step 8400: policy_loss=-2.8423, grad_norm=0.0000, entropy=0.0000
[Training] Step 8500: policy_loss=0.3861, grad_norm=0.0000, entropy=0.0000
[Training] Step 8600: policy_loss=0.5290, grad_norm=0.0000, entropy=0.0000
[Training] Step 8700: policy_loss=-1.3564, grad_norm=0.0000, entropy=0.0000
[Training] Step 8800: policy_loss=-0.4888, grad_norm=0.0000, entropy=0.0000
[Training] Step 8900: policy_loss=-1.6735, grad_norm=0.0000, entropy=0.0000
Traceback (most recent call last):
  File ".\src\main.py", line 511, in <module>
    main()
  File ".\src\main.py", line 507, in main
    train(config, args)
  File ".\src\main.py", line 273, in train
    actions, _ = agent.get_action(obs, deterministic=False)
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\agents.py", line 155, in get_action
    actions, log_probs, entropies, values = self.actor_critic.get_action_and_value(
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\models.py", line 790, in get_action_and_value
    log_prob = dist.log_prob(action)
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\distributions\categorical.py", line 138, in log_prob
    self._validate_sample(value)
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\distributions\distribution.py", line 314, in _validate_sample
    if not valid.all():
KeyboardInterrupt
Traceback (most recent call last):
  File ".\src\main.py", line 511, in <module>
    main()
  File ".\src\main.py", line 507, in main
    train(config, args)
  File ".\src\main.py", line 273, in train
    actions, _ = agent.get_action(obs, deterministic=False)
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\agents.py", line 155, in get_action
    actions, log_probs, entropies, values = self.actor_critic.get_action_and_value(
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\models.py", line 790, in get_action_and_value
    log_prob = dist.log_prob(action)
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\distributions\categorical.py", line 138, in log_prob
    self._validate_sample(value)
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\distributions\distribution.py", line 314, in _validate_sample
    if not valid.all():
KeyboardInterrupt
