wandb: Detected [agents] in use.
wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/

Starting training for 1,000,000 timesteps...
============================================================
[ActionDebug] Step 0: {'action/0': 2, 'action/1': 0, 'action/2': 1, 'action/3': 1}
Step:      1,000 | Episodes:     27 | Mean Reward (100):    0.26 | Best:    -inf
[ActionDebug] Step 1000: {'action/0': 256, 'action/1': 231, 'action/2': 248, 'action/3': 269}
Step:      2,000 | Episodes:     62 | Mean Reward (100):    0.19 | Best:    -inf
[ActionDebug] Step 2000: {'action/0': 499, 'action/1': 451, 'action/2': 495, 'action/3': 559}
Step:      3,000 | Episodes:     93 | Mean Reward (100):    0.19 | Best:    -inf
[ActionDebug] Step 3000: {'action/0': 760, 'action/1': 681, 'action/2': 733, 'action/3': 830}
C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\gymnasium\wrappers\rendering.py:283: UserWarning: [33mWARN: Overwriting existing videos at D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\videos\eval-000100 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  logger.warn(
[Video] Saved episode video: videos\eval-episode-000100.mp4 (reward: 0.0)
Step:      4,000 | Episodes:    122 | Mean Reward (100):    0.21 | Best:    0.23
[ActionDebug] Step 4000: {'action/0': 1030, 'action/1': 946, 'action/2': 961, 'action/3': 1067}
Step:      5,000 | Episodes:    146 | Mean Reward (100):    0.26 | Best:    0.26
[ActionDebug] Step 5000: {'action/0': 1270, 'action/1': 1211, 'action/2': 1218, 'action/3': 1305}
Step:      6,000 | Episodes:    176 | Mean Reward (100):    0.30 | Best:    0.30
[ActionDebug] Step 6000: {'action/0': 1521, 'action/1': 1448, 'action/2': 1477, 'action/3': 1558}
C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\gymnasium\wrappers\rendering.py:283: UserWarning: [33mWARN: Overwriting existing videos at D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\videos\eval-000200 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  logger.warn(
[Video] Saved episode video: videos\eval-episode-000200.mp4 (reward: 0.0)
Step:      7,000 | Episodes:    211 | Mean Reward (100):    0.25 | Best:    0.32
[ActionDebug] Step 7000: {'action/0': 1761, 'action/1': 1712, 'action/2': 1722, 'action/3': 1809}
Step:      8,000 | Episodes:    241 | Mean Reward (100):    0.22 | Best:    0.32
[ActionDebug] Step 8000: {'action/0': 2026, 'action/1': 1945, 'action/2': 1965, 'action/3': 2068}
Step:      9,000 | Episodes:    268 | Mean Reward (100):    0.22 | Best:    0.32
[ActionDebug] Step 9000: {'action/0': 2278, 'action/1': 2191, 'action/2': 2215, 'action/3': 2320}
Step:     10,000 | Episodes:    296 | Mean Reward (100):    0.24 | Best:    0.32
C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\gymnasium\wrappers\rendering.py:283: UserWarning: [33mWARN: Overwriting existing videos at D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\videos\eval-000300 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  logger.warn(
[Video] Saved episode video: videos\eval-episode-000300.mp4 (reward: 0.0)
Step:     11,000 | Episodes:    322 | Mean Reward (100):    0.24 | Best:    0.32
Step:     12,000 | Episodes:    350 | Mean Reward (100):    0.29 | Best:    0.32
Step:     13,000 | Episodes:    379 | Mean Reward (100):    0.29 | Best:    0.32
C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\gymnasium\wrappers\rendering.py:283: UserWarning: [33mWARN: Overwriting existing videos at D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\videos\eval-000400 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  logger.warn(
[Video] Saved episode video: videos\eval-episode-000400.mp4 (reward: 0.0)
Step:     14,000 | Episodes:    408 | Mean Reward (100):    0.28 | Best:    0.32
Step:     15,000 | Episodes:    435 | Mean Reward (100):    0.29 | Best:    0.32
Step:     16,000 | Episodes:    463 | Mean Reward (100):    0.26 | Best:    0.32
Step:     17,000 | Episodes:    488 | Mean Reward (100):    0.29 | Best:    0.32
C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\gymnasium\wrappers\rendering.py:283: UserWarning: [33mWARN: Overwriting existing videos at D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\videos\eval-000500 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  logger.warn(
[Video] Saved episode video: videos\eval-episode-000500.mp4 (reward: 0.0)
Step:     18,000 | Episodes:    512 | Mean Reward (100):    0.36 | Best:    0.37
Step:     19,000 | Episodes:    538 | Mean Reward (100):    0.37 | Best:    0.41
[Training] Step 20000: policy_loss=0.0000, grad_norm=0.4059, entropy=1.3863
Step:     20,000 | Episodes:    576 | Mean Reward (100):    0.30 | Best:    0.41
[Training] Step 20100: policy_loss=-0.0070, grad_norm=0.6082, entropy=1.3860
[Training] Step 20200: policy_loss=0.0058, grad_norm=0.4889, entropy=1.3798
[Training] Step 20300: policy_loss=-0.0614, grad_norm=1.7571, entropy=1.3683
[Training] Step 20400: policy_loss=-0.1112, grad_norm=2.2056, entropy=1.3469
[Training] Step 20500: policy_loss=0.1950, grad_norm=3.2064, entropy=1.2279
[Training] Step 20600: policy_loss=0.1944, grad_norm=3.5448, entropy=0.9664
[Training] Step 20700: policy_loss=-0.2394, grad_norm=9.8827, entropy=0.6352
[Training] Step 20800: policy_loss=-0.2724, grad_norm=18.0851, entropy=0.3691
[Training] Step 20900: policy_loss=-1.7010, grad_norm=12.3711, entropy=0.7053
[Training] Step 21000: policy_loss=0.0022, grad_norm=13.4767, entropy=0.6944
Step:     21,000 | Episodes:    599 | Mean Reward (100):    0.35 | Best:    0.41
C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\gymnasium\wrappers\rendering.py:283: UserWarning: [33mWARN: Overwriting existing videos at D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\videos\eval-000600 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  logger.warn(
[Video] Saved episode video: videos\eval-episode-000600.mp4 (reward: 0.0)
[Training] Step 21100: policy_loss=-0.7603, grad_norm=13.7363, entropy=0.3626
[Training] Step 21200: policy_loss=0.2029, grad_norm=21.0946, entropy=0.1556
[Training] Step 21300: policy_loss=-1.3288, grad_norm=11.0143, entropy=0.0167
[Training] Step 21400: policy_loss=-1.3344, grad_norm=22.5515, entropy=0.0005
[Training] Step 21500: policy_loss=0.6196, grad_norm=11.9788, entropy=0.0000
[Training] Step 21600: policy_loss=-1.2082, grad_norm=5.1736, entropy=0.0000
[Training] Step 21700: policy_loss=-2.4423, grad_norm=0.3121, entropy=0.0000
[Training] Step 21800: policy_loss=-0.1861, grad_norm=22.1278, entropy=0.0000
[Training] Step 21900: policy_loss=0.8727, grad_norm=4.1396, entropy=0.0000
[Training] Step 22000: policy_loss=-0.7679, grad_norm=8.1926, entropy=0.0000
Step:     22,000 | Episodes:    622 | Mean Reward (100):    0.31 | Best:    0.41
[Training] Step 22100: policy_loss=-0.6679, grad_norm=19.1614, entropy=0.0000
[Training] Step 22200: policy_loss=-1.6858, grad_norm=1.2522, entropy=0.0000
[Training] Step 22300: policy_loss=-0.2945, grad_norm=1.0459, entropy=0.0000
Traceback (most recent call last):
  File ".\src\main.py", line 511, in <module>
    main()
  File ".\src\main.py", line 507, in main
    train(config, args)
  File ".\src\main.py", line 359, in train
    update_metrics = agent.update()
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\agents.py", line 385, in update
    policy_metrics = self.update_policy(n_updates=1)
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\agents.py", line 313, in update_policy
    _, _, _, next_values = self.actor_critic.get_action_and_value(
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\models.py", line 780, in get_action_and_value
    logits, value = self.forward(obs)
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\models.py", line 755, in forward
    features = self.encoder(obs)
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\models.py", line 112, in forward
    x = F.relu(self.conv3(x))
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\nn\modules\conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\nn\modules\conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
KeyboardInterrupt
Traceback (most recent call last):
  File ".\src\main.py", line 511, in <module>
    main()
  File ".\src\main.py", line 507, in main
    train(config, args)
  File ".\src\main.py", line 359, in train
    update_metrics = agent.update()
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\agents.py", line 385, in update
    policy_metrics = self.update_policy(n_updates=1)
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\agents.py", line 313, in update_policy
    _, _, _, next_values = self.actor_critic.get_action_and_value(
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\models.py", line 780, in get_action_and_value
    logits, value = self.forward(obs)
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\models.py", line 755, in forward
    features = self.encoder(obs)
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\models.py", line 112, in forward
    x = F.relu(self.conv3(x))
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\nn\modules\conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\nn\modules\conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
KeyboardInterrupt
