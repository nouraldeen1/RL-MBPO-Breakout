wandb: Detected [agents] in use.
wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/

Starting training for 100,000 timesteps...
============================================================
[ActionDebug] Step 0: {'action/0': 2, 'action/1': 0, 'action/2': 1, 'action/3': 1}
Step:      1,000 | Episodes:     27 | Mean Reward (100):    0.26 | Best:    -inf
[ActionDebug] Step 1000: {'action/0': 256, 'action/1': 231, 'action/2': 248, 'action/3': 269}
Step:      2,000 | Episodes:     56 | Mean Reward (100):    0.29 | Best:    -inf
[ActionDebug] Step 2000: {'action/0': 499, 'action/1': 451, 'action/2': 495, 'action/3': 559}
Step:      3,000 | Episodes:     89 | Mean Reward (100):    0.22 | Best:    -inf
[ActionDebug] Step 3000: {'action/0': 760, 'action/1': 681, 'action/2': 733, 'action/3': 830}
C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\gymnasium\wrappers\rendering.py:283: UserWarning: [33mWARN: Overwriting existing videos at D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\videos\eval-000100 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  logger.warn(
[Video] Saved episode video: videos\eval-episode-000100.mp4 (reward: 0.0)
Step:      4,000 | Episodes:    119 | Mean Reward (100):    0.26 | Best:    0.28
[ActionDebug] Step 4000: {'action/0': 1030, 'action/1': 946, 'action/2': 961, 'action/3': 1067}
[Training] Step 5000: policy_loss=0.0000, grad_norm=0.3344, entropy=1.3863
Step:      5,000 | Episodes:    147 | Mean Reward (100):    0.24 | Best:    0.28
[ActionDebug] Step 5000: {'action/0': 1270, 'action/1': 1211, 'action/2': 1218, 'action/3': 1305}
[Training] Step 5100: policy_loss=-0.0021, grad_norm=0.1837, entropy=1.3859
[Training] Step 5200: policy_loss=-0.0037, grad_norm=0.1563, entropy=1.3837
[Training] Step 5300: policy_loss=-0.0021, grad_norm=0.1254, entropy=1.3860
[Training] Step 5400: policy_loss=-0.0076, grad_norm=0.1671, entropy=1.3832
[Training] Step 5500: policy_loss=-2.1538, grad_norm=9.4313, entropy=0.3896
[Training] Step 5600: policy_loss=1.2010, grad_norm=5.0082, entropy=0.4279
[Training] Step 5700: policy_loss=0.5651, grad_norm=13.3478, entropy=0.2041
[Training] Step 5800: policy_loss=-0.4140, grad_norm=3.0833, entropy=0.0003
[Training] Step 5900: policy_loss=-2.4393, grad_norm=0.3336, entropy=0.0000
[Training] Step 6000: policy_loss=-1.0156, grad_norm=15.1655, entropy=0.0000
Step:      6,000 | Episodes:    176 | Mean Reward (100):    0.25 | Best:    0.28
[ActionDebug] Step 6000: {'action/0': 1456, 'action/1': 1310, 'action/2': 1815, 'action/3': 1423}
[Training] Step 6100: policy_loss=-2.6563, grad_norm=0.3301, entropy=0.0000
Traceback (most recent call last):
  File ".\src\main.py", line 511, in <module>
    main()
  File ".\src\main.py", line 507, in main
    train(config, args)
  File ".\src\main.py", line 359, in train
    update_metrics = agent.update()
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\agents.py", line 372, in update
    dynamics_metrics = self.train_dynamics(n_updates=1)
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\agents.py", line 207, in train_dynamics
    loss, metrics = self.dynamics.compute_loss(
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\models.py", line 683, in compute_loss
    pred_delta, pred_reward = model(state, action)
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\models.py", line 459, in forward
    delta_state = self.delta_decoder(transition_features)
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\nn\modules\container.py", line 219, in forward
    input = module(input)
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\nn\modules\module.py", line 1556, in _call_impl
    forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)
KeyboardInterrupt
Traceback (most recent call last):
  File ".\src\main.py", line 511, in <module>
    main()
  File ".\src\main.py", line 507, in main
    train(config, args)
  File ".\src\main.py", line 359, in train
    update_metrics = agent.update()
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\agents.py", line 372, in update
    dynamics_metrics = self.train_dynamics(n_updates=1)
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\agents.py", line 207, in train_dynamics
    loss, metrics = self.dynamics.compute_loss(
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\models.py", line 683, in compute_loss
    pred_delta, pred_reward = model(state, action)
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\models.py", line 459, in forward
    delta_state = self.delta_decoder(transition_features)
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\nn\modules\module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\nn\modules\container.py", line 219, in forward
    input = module(input)
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\nn\modules\module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\torch\nn\modules\module.py", line 1556, in _call_impl
    forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)
KeyboardInterrupt
