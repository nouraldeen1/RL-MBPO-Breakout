wandb: Detected [agents] in use.
wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/

Starting training for 100,000 timesteps...
============================================================
[ActionDebug] Step 0: {'action/0': 4, 'action/1': 0, 'action/2': 0, 'action/3': 0}
Step:      1,000 | Episodes:     29 | Mean Reward (100):    0.21 | Best:    -inf
[ActionDebug] Step 1000: {'action/0': 253, 'action/1': 244, 'action/2': 251, 'action/3': 256}
Step:      2,000 | Episodes:     58 | Mean Reward (100):    0.24 | Best:    -inf
[ActionDebug] Step 2000: {'action/0': 509, 'action/1': 496, 'action/2': 512, 'action/3': 487}
Step:      3,000 | Episodes:     83 | Mean Reward (100):    0.28 | Best:    -inf
[ActionDebug] Step 3000: {'action/0': 748, 'action/1': 760, 'action/2': 778, 'action/3': 718}
C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\gymnasium\wrappers\rendering.py:283: UserWarning: [33mWARN: Overwriting existing videos at D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\videos\eval-000100 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  logger.warn(
[Video] Saved episode video: videos\eval-episode-000100.mp4 (reward: 0.0)
Step:      4,000 | Episodes:    117 | Mean Reward (100):    0.29 | Best:    0.30
[ActionDebug] Step 4000: {'action/0': 987, 'action/1': 1004, 'action/2': 1031, 'action/3': 982}
Step:      5,000 | Episodes:    142 | Mean Reward (100):    0.29 | Best:    0.32
[ActionDebug] Step 5000: {'action/0': 1239, 'action/1': 1244, 'action/2': 1305, 'action/3': 1216}
Step:      6,000 | Episodes:    171 | Mean Reward (100):    0.26 | Best:    0.32
[ActionDebug] Step 6000: {'action/0': 1469, 'action/1': 1506, 'action/2': 1579, 'action/3': 1450}
Step:      7,000 | Episodes:    197 | Mean Reward (100):    0.32 | Best:    0.33
[ActionDebug] Step 7000: {'action/0': 1746, 'action/1': 1750, 'action/2': 1820, 'action/3': 1688}
C:\Users\nourh\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\gymnasium\wrappers\rendering.py:283: UserWarning: [33mWARN: Overwriting existing videos at D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\videos\eval-000200 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)[0m
  logger.warn(
[Video] Saved episode video: videos\eval-episode-000200.mp4 (reward: 2.0)
Step:      8,000 | Episodes:    226 | Mean Reward (100):    0.32 | Best:    0.34
[ActionDebug] Step 8000: {'action/0': 1988, 'action/1': 2005, 'action/2': 2053, 'action/3': 1958}
Step:      9,000 | Episodes:    256 | Mean Reward (100):    0.30 | Best:    0.34
[ActionDebug] Step 9000: {'action/0': 2224, 'action/1': 2251, 'action/2': 2306, 'action/3': 2223}
[Training] Step 10000: policy_loss=-0.0001, grad_norm=0.3062, entropy=1.3863
Step:     10,000 | Episodes:    283 | Mean Reward (100):    0.28 | Best:    0.35
Traceback (most recent call last):
  File ".\src\main.py", line 509, in <module>
    main()
  File ".\src\main.py", line 505, in main
    train(config, args)
  File ".\src\main.py", line 363, in train
    update_metrics = agent.update()
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\agents.py", line 347, in update
    n_generated = self.generate_model_rollouts()
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\agents.py", line 244, in generate_model_rollouts
    next_states, rewards, _ = self.dynamics.predict_next_state(
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\models.py", line 652, in predict_next_state
    next_state = torch.clamp(next_state, 0.0, 1.0)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 18.48 GiB is allocated by PyTorch, and 244.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File ".\src\main.py", line 509, in <module>
    main()
  File ".\src\main.py", line 505, in main
    train(config, args)
  File ".\src\main.py", line 363, in train
    update_metrics = agent.update()
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\agents.py", line 347, in update
    n_generated = self.generate_model_rollouts()
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\agents.py", line 244, in generate_model_rollouts
    next_states, rewards, _ = self.dynamics.predict_next_state(
  File "D:\as3b folder\CCE-C\term 5\rl\RL-last\RL-MBPO-Breakout\src\models.py", line 652, in predict_next_state
    next_state = torch.clamp(next_state, 0.0, 1.0)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 18.48 GiB is allocated by PyTorch, and 244.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
